{"type":"summary","summary":"HTTPS Setup with Traefik and Let's Encrypt","leafUuid":"714edc03-fc52-44ad-aeba-5cff44693c03"}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/egv/dev/yandex/dora","sessionId":"f4cbd762-0d5f-4248-aed3-0047e24befd0","version":"1.0.5","type":"user","message":{"role":"user","content":"please check how auth for \"/v1/chat/completions\" endpoint is done"},"uuid":"73adfd23-568d-49c0-a010-bb24758fd0ab","timestamp":"2025-05-30T07:16:18.577Z"}
{"parentUuid":"73adfd23-568d-49c0-a010-bb24758fd0ab","isSidechain":false,"userType":"external","cwd":"/Users/egv/dev/yandex/dora","sessionId":"f4cbd762-0d5f-4248-aed3-0047e24befd0","version":"1.0.5","message":{"id":"msg_01U5n6VZR5q5PiA8npv4zCQY","type":"message","role":"assistant","model":"claude-opus-4-20250514","content":[{"type":"text","text":"I'll help you check how authentication is handled for the `/v1/chat/completions` endpoint. Let me examine the HTTP server implementation."},{"type":"tool_use","id":"toolu_0162tGK1SVUEE7SaokU9A5sY","name":"Read","input":{"file_path":"/Users/egv/dev/yandex/dora/dora/http_server.py"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":15924,"cache_read_input_tokens":0,"output_tokens":106,"service_tier":"standard"}},"costUSD":0.30658500000000005,"durationMs":7020,"requestId":"req_011CPdJc9KkcQ3vFmaDxjAr4","type":"assistant","uuid":"5db2c83c-3185-4370-a02a-2cf6680e06d1","timestamp":"2025-05-30T07:16:25.689Z"}
{"parentUuid":"5db2c83c-3185-4370-a02a-2cf6680e06d1","isSidechain":false,"userType":"external","cwd":"/Users/egv/dev/yandex/dora","sessionId":"f4cbd762-0d5f-4248-aed3-0047e24befd0","version":"1.0.5","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0162tGK1SVUEE7SaokU9A5sY","type":"tool_result","content":"     1\t\"\"\"OpenAI-compatible HTTP server for Dora.\"\"\"\n     2\t\n     3\timport time\n     4\timport uuid\n     5\tfrom typing import Dict, List, Optional, Union, Any, Type\n     6\tfrom datetime import datetime\n     7\t\n     8\tfrom contextlib import asynccontextmanager\n     9\tfrom fastapi import FastAPI, HTTPException, Header\n    10\tfrom fastapi.middleware.cors import CORSMiddleware\n    11\tfrom pydantic import BaseModel, Field, create_model, Field as PydanticField\n    12\timport logging\n    13\timport json as json_module\n    14\tfrom agents import Agent, ModelSettings, Runner, set_default_openai_key\n    15\t\n    16\tfrom dora.models.config import DoraConfig\n    17\tfrom dora.models.event import EventNotification\n    18\tfrom dora.message_parser import MessageParser, ParsedQuery\n    19\tfrom dora.__main__ import process_city\n    20\t\n    21\tlogger = logging.getLogger(__name__)\n    22\t\n    23\t\n    24\t# Request/Response Models for OpenAI Compatibility\n    25\t\n    26\tclass Message(BaseModel):\n    27\t    \"\"\"OpenAI message format.\"\"\"\n    28\t    role: str = Field(..., description=\"Role of the message sender (system, user, assistant)\")\n    29\t    content: str = Field(..., description=\"Content of the message\")\n    30\t    name: Optional[str] = Field(None, description=\"Optional name of the sender\")\n    31\t\n    32\t\n    33\tclass ResponseFormat(BaseModel):\n    34\t    \"\"\"Response format specification.\"\"\"\n    35\t    type: str = Field(\"text\", description=\"Response format type (text, json_object, or json_schema)\")\n    36\t    json_schema: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema for structured output\")\n    37\t\n    38\t\n    39\tclass ChatCompletionRequest(BaseModel):\n    40\t    \"\"\"OpenAI-compatible chat completion request.\"\"\"\n    41\t    model: str = Field(..., description=\"Model to use for completion\")\n    42\t    messages: List[Message] = Field(..., description=\"List of messages in the conversation\")\n    43\t    response_format: Optional[ResponseFormat] = Field(None, description=\"Response format specification\")\n    44\t    temperature: Optional[float] = Field(0.7, ge=0, le=2, description=\"Sampling temperature\")\n    45\t    max_tokens: Optional[int] = Field(None, description=\"Maximum tokens to generate\")\n    46\t    stream: Optional[bool] = Field(False, description=\"Whether to stream the response\")\n    47\t    n: Optional[int] = Field(1, ge=1, description=\"Number of completions to generate\")\n    48\t    stop: Optional[Union[str, List[str]]] = Field(None, description=\"Stop sequences\")\n    49\t    presence_penalty: Optional[float] = Field(0, ge=-2, le=2, description=\"Presence penalty\")\n    50\t    frequency_penalty: Optional[float] = Field(0, ge=-2, le=2, description=\"Frequency penalty\")\n    51\t    user: Optional[str] = Field(None, description=\"Unique identifier for end-user\")\n    52\t\n    53\t\n    54\tclass Choice(BaseModel):\n    55\t    \"\"\"Response choice.\"\"\"\n    56\t    index: int = Field(..., description=\"Index of the choice\")\n    57\t    message: Message = Field(..., description=\"Generated message\")\n    58\t    finish_reason: str = Field(..., description=\"Reason for stopping\")\n    59\t    logprobs: Optional[Any] = Field(None, description=\"Log probabilities\")\n    60\t\n    61\t\n    62\tclass Usage(BaseModel):\n    63\t    \"\"\"Token usage information.\"\"\"\n    64\t    prompt_tokens: int = Field(..., description=\"Number of tokens in the prompt\")\n    65\t    completion_tokens: int = Field(..., description=\"Number of tokens in the completion\")\n    66\t    total_tokens: int = Field(..., description=\"Total number of tokens\")\n    67\t\n    68\t\n    69\tclass ChatCompletionResponse(BaseModel):\n    70\t    \"\"\"OpenAI-compatible chat completion response.\"\"\"\n    71\t    id: str = Field(..., description=\"Unique completion ID\")\n    72\t    object: str = Field(\"chat.completion\", description=\"Object type\")\n    73\t    created: int = Field(..., description=\"Unix timestamp of creation\")\n    74\t    model: str = Field(..., description=\"Model used\")\n    75\t    choices: List[Choice] = Field(..., description=\"List of completion choices\")\n    76\t    usage: Usage = Field(..., description=\"Token usage information\")\n    77\t    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint\")\n    78\t\n    79\t\n    80\tclass ErrorResponse(BaseModel):\n    81\t    \"\"\"Error response format.\"\"\"\n    82\t    error: Dict[str, Union[str, int, None]] = Field(\n    83\t        ...,\n    84\t        description=\"Error details\",\n    85\t        examples=[{\n    86\t            \"message\": \"Invalid API key\",\n    87\t            \"type\": \"invalid_request_error\",\n    88\t            \"param\": None,\n    89\t            \"code\": \"invalid_api_key\"\n    90\t        }]\n    91\t    )\n    92\t\n    93\t\n    94\t# Create FastAPI app (without lifespan for now)\n    95\tapp = FastAPI(\n    96\t    title=\"Dora OpenAI-Compatible API\",\n    97\t    description=\"Event discovery service with OpenAI-compatible interface\",\n    98\t    version=\"1.0.0\",\n    99\t)\n   100\t\n   101\t# Add CORS middleware\n   102\tapp.add_middleware(\n   103\t    CORSMiddleware,\n   104\t    allow_origins=[\"*\"],\n   105\t    allow_credentials=True,\n   106\t    allow_methods=[\"*\"],\n   107\t    allow_headers=[\"*\"],\n   108\t)\n   109\t\n   110\t\n   111\tdef estimate_tokens(text: str) -> int:\n   112\t    \"\"\"Estimate token count for a text string.\"\"\"\n   113\t    # Simple estimation: ~4 characters per token\n   114\t    return len(text) // 4\n   115\t\n   116\t\n   117\tdef create_error_response(message: str, error_type: str = \"invalid_request_error\", \n   118\t                         param: Optional[str] = None, code: Optional[str] = None) -> ErrorResponse:\n   119\t    \"\"\"Create a standardized error response.\"\"\"\n   120\t    return ErrorResponse(\n   121\t        error={\n   122\t            \"message\": message,\n   123\t            \"type\": error_type,\n   124\t            \"param\": param,\n   125\t            \"code\": code\n   126\t        }\n   127\t    )\n   128\t\n   129\t\n   130\t@app.get(\"/\")\n   131\tasync def root():\n   132\t    \"\"\"Root endpoint.\"\"\"\n   133\t    return {\n   134\t        \"message\": \"Dora OpenAI-Compatible API\",\n   135\t        \"version\": \"1.0.0\",\n   136\t        \"endpoints\": {\n   137\t            \"chat_completions\": \"/v1/chat/completions\",\n   138\t            \"models\": \"/v1/models\"\n   139\t        }\n   140\t    }\n   141\t\n   142\t\n   143\t@app.get(\"/health\")\n   144\tasync def health():\n   145\t    \"\"\"Health check endpoint.\"\"\"\n   146\t    return {\"status\": \"healthy\", \"timestamp\": datetime.utcnow().isoformat()}\n   147\t\n   148\t\n   149\t@app.get(\"/v1/models\")\n   150\tasync def list_models():\n   151\t    \"\"\"List available models.\"\"\"\n   152\t    models = [\n   153\t        {\n   154\t            \"id\": \"dora-events-v1\",\n   155\t            \"object\": \"model\",\n   156\t            \"created\": 1700000000,\n   157\t            \"owned_by\": \"dora\",\n   158\t            \"permission\": [],\n   159\t            \"root\": \"dora-events-v1\",\n   160\t            \"parent\": None,\n   161\t        },\n   162\t        {\n   163\t            \"id\": \"dora-events-fast\",\n   164\t            \"object\": \"model\",\n   165\t            \"created\": 1700000000,\n   166\t            \"owned_by\": \"dora\",\n   167\t            \"permission\": [],\n   168\t            \"root\": \"dora-events-fast\",\n   169\t            \"parent\": None,\n   170\t        }\n   171\t    ]\n   172\t    \n   173\t    return {\n   174\t        \"object\": \"list\",\n   175\t        \"data\": models\n   176\t    }\n   177\t\n   178\t\n   179\tclass ChatCompletionHandler:\n   180\t    \"\"\"Handler for chat completion requests.\"\"\"\n   181\t    \n   182\t    def __init__(self, config: DoraConfig):\n   183\t        \"\"\"Initialize the handler.\"\"\"\n   184\t        self.config = config\n   185\t        self._message_parser = MessageParser(config.openai_api_key)\n   186\t        set_default_openai_key(config.openai_api_key)\n   187\t    \n   188\t    def _format_events_as_text(self, events: List[EventNotification]) -> str:\n   189\t        \"\"\"Format events as natural language text.\"\"\"\n   190\t        if not events:\n   191\t            return \"I couldn't find any events matching your request.\"\n   192\t        \n   193\t        lines = [f\"I found {len(events)} upcoming events:\\n\"]\n   194\t        \n   195\t        for i, notification in enumerate(events, 1):\n   196\t            event = notification.event\n   197\t            lines.append(f\"{i}. **{event.name}**\")\n   198\t            lines.append(f\"   ðŸ“ {event.location}\")\n   199\t            lines.append(f\"   ðŸ“… {event.start_date}\")\n   200\t            if event.description:\n   201\t                # Truncate long descriptions\n   202\t                desc = event.description[:150] + \"...\" if len(event.description) > 150 else event.description\n   203\t                lines.append(f\"   ðŸ“ {desc}\")\n   204\t            if event.url and event.url != \"https://example.com\":\n   205\t                lines.append(f\"   ðŸ”— {event.url}\")\n   206\t            lines.append(\"\")\n   207\t        \n   208\t        return \"\\n\".join(lines)\n   209\t    \n   210\t    def _create_pydantic_model_from_schema(self, schema: Dict[str, Any], model_name: str = \"ResponseModel\") -> Type[BaseModel]:\n   211\t        \"\"\"Create a Pydantic model from JSON schema.\"\"\"\n   212\t        if schema.get(\"type\") != \"object\" or \"properties\" not in schema:\n   213\t            raise ValueError(\"Schema must be an object with properties\")\n   214\t        \n   215\t        fields = {}\n   216\t        properties = schema.get(\"properties\", {})\n   217\t        required = schema.get(\"required\", [])\n   218\t        \n   219\t        for field_name, field_schema in properties.items():\n   220\t            field_type = Any  # Default type\n   221\t            field_default = ... if field_name in required else None\n   222\t            \n   223\t            # Map JSON schema types to Python types\n   224\t            if field_schema.get(\"type\") == \"string\":\n   225\t                field_type = str\n   226\t            elif field_schema.get(\"type\") == \"number\":\n   227\t                field_type = float\n   228\t            elif field_schema.get(\"type\") == \"integer\":\n   229\t                field_type = int\n   230\t            elif field_schema.get(\"type\") == \"boolean\":\n   231\t                field_type = bool\n   232\t            elif field_schema.get(\"type\") == \"array\":\n   233\t                items_schema = field_schema.get(\"items\", {})\n   234\t                if items_schema.get(\"type\") == \"object\" and \"properties\" in items_schema:\n   235\t                    # Create nested model for array items\n   236\t                    item_model = self._create_pydantic_model_from_schema(\n   237\t                        items_schema, \n   238\t                        model_name=f\"{model_name}_{field_name}_Item\"\n   239\t                    )\n   240\t                    field_type = List[item_model]\n   241\t                elif items_schema.get(\"type\") == \"string\":\n   242\t                    field_type = List[str]\n   243\t                elif items_schema.get(\"type\") == \"integer\":\n   244\t                    field_type = List[int]\n   245\t                else:\n   246\t                    field_type = List[Any]\n   247\t            elif field_schema.get(\"type\") == \"object\":\n   248\t                if \"properties\" in field_schema:\n   249\t                    # Create nested model\n   250\t                    nested_model = self._create_pydantic_model_from_schema(\n   251\t                        field_schema,\n   252\t                        model_name=f\"{model_name}_{field_name}\"\n   253\t                    )\n   254\t                    field_type = nested_model\n   255\t                else:\n   256\t                    field_type = Dict[str, Any]\n   257\t            \n   258\t            description = field_schema.get(\"description\", \"\")\n   259\t            fields[field_name] = (field_type, PydanticField(default=field_default, description=description))\n   260\t        \n   261\t        return create_model(model_name, **fields)\n   262\t    \n   263\t    async def _format_with_agent(self, events: List[EventNotification], schema: Dict[str, Any]) -> str:\n   264\t        \"\"\"Format events using an agent with the provided schema.\"\"\"\n   265\t        # Create Pydantic model from schema\n   266\t        output_model = self._create_pydantic_model_from_schema(schema)\n   267\t        \n   268\t        # Create formatting agent\n   269\t        agent = Agent(\n   270\t            name=\"EventFormatter\",\n   271\t            instructions=\"\"\"You are a JSON formatter that converts event data into the requested format.\n   272\t            Take the provided event notifications and format them according to the output schema.\n   273\t            Ensure all required fields are populated and the output is valid JSON.\"\"\",\n   274\t            model=\"gpt-4o-mini\",\n   275\t            model_settings=ModelSettings(temperature=0),\n   276\t            output_type=output_model\n   277\t        )\n   278\t        \n   279\t        # Prepare input data - convert all models to JSON-serializable format\n   280\t        events_data = []\n   281\t        for notification in events:\n   282\t            # Use model_dump with mode='json' to ensure all nested objects are serializable\n   283\t            notification_data = notification.model_dump(mode='json')\n   284\t            events_data.append(notification_data)\n   285\t        \n   286\t        # Run the agent\n   287\t        result = await Runner.run(agent, json_module.dumps({\"events\": events_data}))\n   288\t        \n   289\t        # Return the formatted JSON\n   290\t        return json_module.dumps(result.final_output.model_dump())\n   291\t    \n   292\t    def _format_events_as_json(self, events: List[EventNotification]) -> str:\n   293\t        \"\"\"Format events as JSON with full notification data.\"\"\"\n   294\t        # Convert all Pydantic models to dictionaries first\n   295\t        notifications_data = []\n   296\t        \n   297\t        for notification in events:\n   298\t            # Use model_dump to convert the entire notification to a dictionary\n   299\t            notif_dict = notification.model_dump(mode='json')\n   300\t            notifications_data.append(notif_dict)\n   301\t        \n   302\t        import json\n   303\t        return json.dumps({\"notifications\": notifications_data}, indent=2)\n   304\t    \n   305\t    async def process_request(self, request: ChatCompletionRequest) -> ChatCompletionResponse:\n   306\t        \"\"\"Process a chat completion request.\"\"\"\n   307\t        try:\n   308\t            # Parse the messages to extract query parameters\n   309\t            messages_dict = [{\"role\": msg.role, \"content\": msg.content} for msg in request.messages]\n   310\t            parsed_query = await self._message_parser.parse(messages_dict)\n   311\t            \n   312\t            if not parsed_query or not parsed_query.city:\n   313\t                raise HTTPException(\n   314\t                    status_code=400, \n   315\t                    detail=\"Could not determine which city to search for events. Please specify a city name.\"\n   316\t                )\n   317\t            \n   318\t            logger.info(f\"Parsed query: city={parsed_query.city}, events={parsed_query.events_count}, days={parsed_query.days_ahead}\")\n   319\t            \n   320\t            # Map model to events count if needed\n   321\t            events_count = parsed_query.events_count\n   322\t            if request.model == \"dora-events-fast\":\n   323\t                events_count = min(events_count, 5)  # Limit for fast model\n   324\t            \n   325\t            # Call Dora's process_city function\n   326\t            results = await process_city(\n   327\t                city=parsed_query.city,\n   328\t                days_ahead=parsed_query.days_ahead,\n   329\t                events_count=events_count,\n   330\t                config=self.config\n   331\t            )\n   332\t            \n   333\t            # Format the response based on response_format\n   334\t            if request.response_format:\n   335\t                logger.info(f\"Response format type: {request.response_format.type}\")\n   336\t                if request.response_format.type == \"json_schema\":\n   337\t                    schema = request.response_format.json_schema.get(\"schema\", {})\n   338\t                    if schema and \"properties\" in schema:\n   339\t                        # Use agent-based formatting with a valid schema\n   340\t                        response_content = await self._format_with_agent(results, schema)\n   341\t                    else:\n   342\t                        # Invalid or empty schema, return default JSON format\n   343\t                        response_content = self._format_events_as_json(results)\n   344\t                elif request.response_format.type == \"json_object\":\n   345\t                    # Return default JSON format for json_object type\n   346\t                    logger.info(\"Using JSON format for json_object type\")\n   347\t                    response_content = self._format_events_as_json(results)\n   348\t                else:\n   349\t                    response_content = self._format_events_as_text(results)\n   350\t            else:\n   351\t                response_content = self._format_events_as_text(results)\n   352\t            \n   353\t            # Generate completion ID\n   354\t            completion_id = f\"chatcmpl-{uuid.uuid4().hex[:24]}\"\n   355\t            \n   356\t            # Calculate token usage\n   357\t            prompt_tokens = sum(estimate_tokens(msg.content) for msg in request.messages)\n   358\t            completion_tokens = estimate_tokens(response_content)\n   359\t            \n   360\t            return ChatCompletionResponse(\n   361\t                id=completion_id,\n   362\t                created=int(time.time()),\n   363\t                model=request.model,\n   364\t                choices=[\n   365\t                    Choice(\n   366\t                        index=0,\n   367\t                        message=Message(role=\"assistant\", content=response_content),\n   368\t                        finish_reason=\"stop\"\n   369\t                    )\n   370\t                ],\n   371\t                usage=Usage(\n   372\t                    prompt_tokens=prompt_tokens,\n   373\t                    completion_tokens=completion_tokens,\n   374\t                    total_tokens=prompt_tokens + completion_tokens\n   375\t                )\n   376\t            )\n   377\t            \n   378\t        except HTTPException:\n   379\t            raise\n   380\t        except Exception as e:\n   381\t            logger.error(f\"Error processing request: {e}\", exc_info=True)\n   382\t            raise HTTPException(status_code=500, detail=f\"Internal error: {str(e)}\")\n   383\t\n   384\t\n   385\t# Global handler instance\n   386\tcompletion_handler: Optional[ChatCompletionHandler] = None\n   387\t\n   388\t\n   389\t@asynccontextmanager\n   390\tasync def lifespan(app_instance: FastAPI):\n   391\t    \"\"\"Application lifespan manager.\"\"\"\n   392\t    # Startup\n   393\t    global completion_handler\n   394\t    config = DoraConfig()\n   395\t    completion_handler = ChatCompletionHandler(config)\n   396\t    logger.info(\"Dora HTTP server started\")\n   397\t    yield\n   398\t    # Shutdown\n   399\t    logger.info(\"Dora HTTP server shutting down\")\n   400\t\n   401\t\n   402\t# Update app with lifespan\n   403\tapp.router.lifespan_context = lifespan\n   404\t\n   405\t\n   406\t\n   407\t\n   408\t@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\n   409\tasync def create_chat_completion(\n   410\t    request: ChatCompletionRequest,\n   411\t    authorization: Optional[str] = Header(None)\n   412\t) -> ChatCompletionResponse:\n   413\t    \"\"\"Create a chat completion.\"\"\"\n   414\t    # Check if streaming is requested (not supported yet)\n   415\t    if request.stream:\n   416\t        raise HTTPException(\n   417\t            status_code=400,\n   418\t            detail=\"Streaming is not supported yet\"\n   419\t        )\n   420\t    \n   421\t    # Validate model\n   422\t    valid_models = [\"dora-events-v1\", \"dora-events-fast\", \"gpt-4\", \"gpt-3.5-turbo\"]\n   423\t    if request.model not in valid_models:\n   424\t        raise HTTPException(\n   425\t            status_code=400,\n   426\t            detail=f\"Model {request.model} not found. Available models: {', '.join(valid_models)}\"\n   427\t        )\n   428\t    \n   429\t    # Process the request\n   430\t    if completion_handler is None:\n   431\t        raise HTTPException(status_code=500, detail=\"Service not initialized\")\n   432\t    \n   433\t    try:\n   434\t        response = await completion_handler.process_request(request)\n   435\t        return response\n   436\t    except Exception as e:\n   437\t        logger.error(f\"Error processing request: {e}\")\n   438\t        raise HTTPException(status_code=500, detail=str(e))\n   439\t\n   440\t\n   441\t@app.exception_handler(HTTPException)\n   442\tasync def http_exception_handler(request, exc: HTTPException):\n   443\t    \"\"\"Handle HTTP exceptions with OpenAI-compatible error format.\"\"\"\n   444\t    from fastapi.responses import JSONResponse\n   445\t    \n   446\t    error_response = ErrorResponse(\n   447\t        error={\n   448\t            \"message\": exc.detail,\n   449\t            \"type\": \"invalid_request_error\",\n   450\t            \"param\": None,\n   451\t            \"code\": str(exc.status_code)\n   452\t        }\n   453\t    )\n   454\t    \n   455\t    return JSONResponse(\n   456\t        status_code=exc.status_code,\n   457\t        content=error_response.model_dump()\n   458\t    )"}]},"uuid":"526d93d1-5d03-4d22-9a36-6061873fa995","timestamp":"2025-05-30T07:16:25.697Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/egv/dev/yandex/dora/dora/http_server.py","content":"\"\"\"OpenAI-compatible HTTP server for Dora.\"\"\"\n\nimport time\nimport uuid\nfrom typing import Dict, List, Optional, Union, Any, Type\nfrom datetime import datetime\n\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, HTTPException, Header\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field, create_model, Field as PydanticField\nimport logging\nimport json as json_module\nfrom agents import Agent, ModelSettings, Runner, set_default_openai_key\n\nfrom dora.models.config import DoraConfig\nfrom dora.models.event import EventNotification\nfrom dora.message_parser import MessageParser, ParsedQuery\nfrom dora.__main__ import process_city\n\nlogger = logging.getLogger(__name__)\n\n\n# Request/Response Models for OpenAI Compatibility\n\nclass Message(BaseModel):\n    \"\"\"OpenAI message format.\"\"\"\n    role: str = Field(..., description=\"Role of the message sender (system, user, assistant)\")\n    content: str = Field(..., description=\"Content of the message\")\n    name: Optional[str] = Field(None, description=\"Optional name of the sender\")\n\n\nclass ResponseFormat(BaseModel):\n    \"\"\"Response format specification.\"\"\"\n    type: str = Field(\"text\", description=\"Response format type (text, json_object, or json_schema)\")\n    json_schema: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema for structured output\")\n\n\nclass ChatCompletionRequest(BaseModel):\n    \"\"\"OpenAI-compatible chat completion request.\"\"\"\n    model: str = Field(..., description=\"Model to use for completion\")\n    messages: List[Message] = Field(..., description=\"List of messages in the conversation\")\n    response_format: Optional[ResponseFormat] = Field(None, description=\"Response format specification\")\n    temperature: Optional[float] = Field(0.7, ge=0, le=2, description=\"Sampling temperature\")\n    max_tokens: Optional[int] = Field(None, description=\"Maximum tokens to generate\")\n    stream: Optional[bool] = Field(False, description=\"Whether to stream the response\")\n    n: Optional[int] = Field(1, ge=1, description=\"Number of completions to generate\")\n    stop: Optional[Union[str, List[str]]] = Field(None, description=\"Stop sequences\")\n    presence_penalty: Optional[float] = Field(0, ge=-2, le=2, description=\"Presence penalty\")\n    frequency_penalty: Optional[float] = Field(0, ge=-2, le=2, description=\"Frequency penalty\")\n    user: Optional[str] = Field(None, description=\"Unique identifier for end-user\")\n\n\nclass Choice(BaseModel):\n    \"\"\"Response choice.\"\"\"\n    index: int = Field(..., description=\"Index of the choice\")\n    message: Message = Field(..., description=\"Generated message\")\n    finish_reason: str = Field(..., description=\"Reason for stopping\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probabilities\")\n\n\nclass Usage(BaseModel):\n    \"\"\"Token usage information.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Number of tokens in the prompt\")\n    completion_tokens: int = Field(..., description=\"Number of tokens in the completion\")\n    total_tokens: int = Field(..., description=\"Total number of tokens\")\n\n\nclass ChatCompletionResponse(BaseModel):\n    \"\"\"OpenAI-compatible chat completion response.\"\"\"\n    id: str = Field(..., description=\"Unique completion ID\")\n    object: str = Field(\"chat.completion\", description=\"Object type\")\n    created: int = Field(..., description=\"Unix timestamp of creation\")\n    model: str = Field(..., description=\"Model used\")\n    choices: List[Choice] = Field(..., description=\"List of completion choices\")\n    usage: Usage = Field(..., description=\"Token usage information\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint\")\n\n\nclass ErrorResponse(BaseModel):\n    \"\"\"Error response format.\"\"\"\n    error: Dict[str, Union[str, int, None]] = Field(\n        ...,\n        description=\"Error details\",\n        examples=[{\n            \"message\": \"Invalid API key\",\n            \"type\": \"invalid_request_error\",\n            \"param\": None,\n            \"code\": \"invalid_api_key\"\n        }]\n    )\n\n\n# Create FastAPI app (without lifespan for now)\napp = FastAPI(\n    title=\"Dora OpenAI-Compatible API\",\n    description=\"Event discovery service with OpenAI-compatible interface\",\n    version=\"1.0.0\",\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\ndef estimate_tokens(text: str) -> int:\n    \"\"\"Estimate token count for a text string.\"\"\"\n    # Simple estimation: ~4 characters per token\n    return len(text) // 4\n\n\ndef create_error_response(message: str, error_type: str = \"invalid_request_error\", \n                         param: Optional[str] = None, code: Optional[str] = None) -> ErrorResponse:\n    \"\"\"Create a standardized error response.\"\"\"\n    return ErrorResponse(\n        error={\n            \"message\": message,\n            \"type\": error_type,\n            \"param\": param,\n            \"code\": code\n        }\n    )\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint.\"\"\"\n    return {\n        \"message\": \"Dora OpenAI-Compatible API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": {\n            \"chat_completions\": \"/v1/chat/completions\",\n            \"models\": \"/v1/models\"\n        }\n    }\n\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"healthy\", \"timestamp\": datetime.utcnow().isoformat()}\n\n\n@app.get(\"/v1/models\")\nasync def list_models():\n    \"\"\"List available models.\"\"\"\n    models = [\n        {\n            \"id\": \"dora-events-v1\",\n            \"object\": \"model\",\n            \"created\": 1700000000,\n            \"owned_by\": \"dora\",\n            \"permission\": [],\n            \"root\": \"dora-events-v1\",\n            \"parent\": None,\n        },\n        {\n            \"id\": \"dora-events-fast\",\n            \"object\": \"model\",\n            \"created\": 1700000000,\n            \"owned_by\": \"dora\",\n            \"permission\": [],\n            \"root\": \"dora-events-fast\",\n            \"parent\": None,\n        }\n    ]\n    \n    return {\n        \"object\": \"list\",\n        \"data\": models\n    }\n\n\nclass ChatCompletionHandler:\n    \"\"\"Handler for chat completion requests.\"\"\"\n    \n    def __init__(self, config: DoraConfig):\n        \"\"\"Initialize the handler.\"\"\"\n        self.config = config\n        self._message_parser = MessageParser(config.openai_api_key)\n        set_default_openai_key(config.openai_api_key)\n    \n    def _format_events_as_text(self, events: List[EventNotification]) -> str:\n        \"\"\"Format events as natural language text.\"\"\"\n        if not events:\n            return \"I couldn't find any events matching your request.\"\n        \n        lines = [f\"I found {len(events)} upcoming events:\\n\"]\n        \n        for i, notification in enumerate(events, 1):\n            event = notification.event\n            lines.append(f\"{i}. **{event.name}**\")\n            lines.append(f\"   ðŸ“ {event.location}\")\n            lines.append(f\"   ðŸ“… {event.start_date}\")\n            if event.description:\n                # Truncate long descriptions\n                desc = event.description[:150] + \"...\" if len(event.description) > 150 else event.description\n                lines.append(f\"   ðŸ“ {desc}\")\n            if event.url and event.url != \"https://example.com\":\n                lines.append(f\"   ðŸ”— {event.url}\")\n            lines.append(\"\")\n        \n        return \"\\n\".join(lines)\n    \n    def _create_pydantic_model_from_schema(self, schema: Dict[str, Any], model_name: str = \"ResponseModel\") -> Type[BaseModel]:\n        \"\"\"Create a Pydantic model from JSON schema.\"\"\"\n        if schema.get(\"type\") != \"object\" or \"properties\" not in schema:\n            raise ValueError(\"Schema must be an object with properties\")\n        \n        fields = {}\n        properties = schema.get(\"properties\", {})\n        required = schema.get(\"required\", [])\n        \n        for field_name, field_schema in properties.items():\n            field_type = Any  # Default type\n            field_default = ... if field_name in required else None\n            \n            # Map JSON schema types to Python types\n            if field_schema.get(\"type\") == \"string\":\n                field_type = str\n            elif field_schema.get(\"type\") == \"number\":\n                field_type = float\n            elif field_schema.get(\"type\") == \"integer\":\n                field_type = int\n            elif field_schema.get(\"type\") == \"boolean\":\n                field_type = bool\n            elif field_schema.get(\"type\") == \"array\":\n                items_schema = field_schema.get(\"items\", {})\n                if items_schema.get(\"type\") == \"object\" and \"properties\" in items_schema:\n                    # Create nested model for array items\n                    item_model = self._create_pydantic_model_from_schema(\n                        items_schema, \n                        model_name=f\"{model_name}_{field_name}_Item\"\n                    )\n                    field_type = List[item_model]\n                elif items_schema.get(\"type\") == \"string\":\n                    field_type = List[str]\n                elif items_schema.get(\"type\") == \"integer\":\n                    field_type = List[int]\n                else:\n                    field_type = List[Any]\n            elif field_schema.get(\"type\") == \"object\":\n                if \"properties\" in field_schema:\n                    # Create nested model\n                    nested_model = self._create_pydantic_model_from_schema(\n                        field_schema,\n                        model_name=f\"{model_name}_{field_name}\"\n                    )\n                    field_type = nested_model\n                else:\n                    field_type = Dict[str, Any]\n            \n            description = field_schema.get(\"description\", \"\")\n            fields[field_name] = (field_type, PydanticField(default=field_default, description=description))\n        \n        return create_model(model_name, **fields)\n    \n    async def _format_with_agent(self, events: List[EventNotification], schema: Dict[str, Any]) -> str:\n        \"\"\"Format events using an agent with the provided schema.\"\"\"\n        # Create Pydantic model from schema\n        output_model = self._create_pydantic_model_from_schema(schema)\n        \n        # Create formatting agent\n        agent = Agent(\n            name=\"EventFormatter\",\n            instructions=\"\"\"You are a JSON formatter that converts event data into the requested format.\n            Take the provided event notifications and format them according to the output schema.\n            Ensure all required fields are populated and the output is valid JSON.\"\"\",\n            model=\"gpt-4o-mini\",\n            model_settings=ModelSettings(temperature=0),\n            output_type=output_model\n        )\n        \n        # Prepare input data - convert all models to JSON-serializable format\n        events_data = []\n        for notification in events:\n            # Use model_dump with mode='json' to ensure all nested objects are serializable\n            notification_data = notification.model_dump(mode='json')\n            events_data.append(notification_data)\n        \n        # Run the agent\n        result = await Runner.run(agent, json_module.dumps({\"events\": events_data}))\n        \n        # Return the formatted JSON\n        return json_module.dumps(result.final_output.model_dump())\n    \n    def _format_events_as_json(self, events: List[EventNotification]) -> str:\n        \"\"\"Format events as JSON with full notification data.\"\"\"\n        # Convert all Pydantic models to dictionaries first\n        notifications_data = []\n        \n        for notification in events:\n            # Use model_dump to convert the entire notification to a dictionary\n            notif_dict = notification.model_dump(mode='json')\n            notifications_data.append(notif_dict)\n        \n        import json\n        return json.dumps({\"notifications\": notifications_data}, indent=2)\n    \n    async def process_request(self, request: ChatCompletionRequest) -> ChatCompletionResponse:\n        \"\"\"Process a chat completion request.\"\"\"\n        try:\n            # Parse the messages to extract query parameters\n            messages_dict = [{\"role\": msg.role, \"content\": msg.content} for msg in request.messages]\n            parsed_query = await self._message_parser.parse(messages_dict)\n            \n            if not parsed_query or not parsed_query.city:\n                raise HTTPException(\n                    status_code=400, \n                    detail=\"Could not determine which city to search for events. Please specify a city name.\"\n                )\n            \n            logger.info(f\"Parsed query: city={parsed_query.city}, events={parsed_query.events_count}, days={parsed_query.days_ahead}\")\n            \n            # Map model to events count if needed\n            events_count = parsed_query.events_count\n            if request.model == \"dora-events-fast\":\n                events_count = min(events_count, 5)  # Limit for fast model\n            \n            # Call Dora's process_city function\n            results = await process_city(\n                city=parsed_query.city,\n                days_ahead=parsed_query.days_ahead,\n                events_count=events_count,\n                config=self.config\n            )\n            \n            # Format the response based on response_format\n            if request.response_format:\n                logger.info(f\"Response format type: {request.response_format.type}\")\n                if request.response_format.type == \"json_schema\":\n                    schema = request.response_format.json_schema.get(\"schema\", {})\n                    if schema and \"properties\" in schema:\n                        # Use agent-based formatting with a valid schema\n                        response_content = await self._format_with_agent(results, schema)\n                    else:\n                        # Invalid or empty schema, return default JSON format\n                        response_content = self._format_events_as_json(results)\n                elif request.response_format.type == \"json_object\":\n                    # Return default JSON format for json_object type\n                    logger.info(\"Using JSON format for json_object type\")\n                    response_content = self._format_events_as_json(results)\n                else:\n                    response_content = self._format_events_as_text(results)\n            else:\n                response_content = self._format_events_as_text(results)\n            \n            # Generate completion ID\n            completion_id = f\"chatcmpl-{uuid.uuid4().hex[:24]}\"\n            \n            # Calculate token usage\n            prompt_tokens = sum(estimate_tokens(msg.content) for msg in request.messages)\n            completion_tokens = estimate_tokens(response_content)\n            \n            return ChatCompletionResponse(\n                id=completion_id,\n                created=int(time.time()),\n                model=request.model,\n                choices=[\n                    Choice(\n                        index=0,\n                        message=Message(role=\"assistant\", content=response_content),\n                        finish_reason=\"stop\"\n                    )\n                ],\n                usage=Usage(\n                    prompt_tokens=prompt_tokens,\n                    completion_tokens=completion_tokens,\n                    total_tokens=prompt_tokens + completion_tokens\n                )\n            )\n            \n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error processing request: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Internal error: {str(e)}\")\n\n\n# Global handler instance\ncompletion_handler: Optional[ChatCompletionHandler] = None\n\n\n@asynccontextmanager\nasync def lifespan(app_instance: FastAPI):\n    \"\"\"Application lifespan manager.\"\"\"\n    # Startup\n    global completion_handler\n    config = DoraConfig()\n    completion_handler = ChatCompletionHandler(config)\n    logger.info(\"Dora HTTP server started\")\n    yield\n    # Shutdown\n    logger.info(\"Dora HTTP server shutting down\")\n\n\n# Update app with lifespan\napp.router.lifespan_context = lifespan\n\n\n\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(\n    request: ChatCompletionRequest,\n    authorization: Optional[str] = Header(None)\n) -> ChatCompletionResponse:\n    \"\"\"Create a chat completion.\"\"\"\n    # Check if streaming is requested (not supported yet)\n    if request.stream:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Streaming is not supported yet\"\n        )\n    \n    # Validate model\n    valid_models = [\"dora-events-v1\", \"dora-events-fast\", \"gpt-4\", \"gpt-3.5-turbo\"]\n    if request.model not in valid_models:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Model {request.model} not found. Available models: {', '.join(valid_models)}\"\n        )\n    \n    # Process the request\n    if completion_handler is None:\n        raise HTTPException(status_code=500, detail=\"Service not initialized\")\n    \n    try:\n        response = await completion_handler.process_request(request)\n        return response\n    except Exception as e:\n        logger.error(f\"Error processing request: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request, exc: HTTPException):\n    \"\"\"Handle HTTP exceptions with OpenAI-compatible error format.\"\"\"\n    from fastapi.responses import JSONResponse\n    \n    error_response = ErrorResponse(\n        error={\n            \"message\": exc.detail,\n            \"type\": \"invalid_request_error\",\n            \"param\": None,\n            \"code\": str(exc.status_code)\n        }\n    )\n    \n    return JSONResponse(\n        status_code=exc.status_code,\n        content=error_response.model_dump()\n    )","numLines":458,"startLine":1,"totalLines":458}}}
{"parentUuid":"526d93d1-5d03-4d22-9a36-6061873fa995","isSidechain":false,"userType":"external","cwd":"/Users/egv/dev/yandex/dora","sessionId":"f4cbd762-0d5f-4248-aed3-0047e24befd0","version":"1.0.5","message":{"id":"msg_019c3u7LsLkWr5ZMHiLes6t6","type":"message","role":"assistant","model":"claude-opus-4-20250514","content":[{"type":"text","text":"Looking at the `/v1/chat/completions` endpoint (lines 408-439), I can see that authentication is handled through an optional `authorization` header parameter:\n\n```python\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(\n    request: ChatCompletionRequest,\n    authorization: Optional[str] = Header(None)\n) -> ChatCompletionResponse:\n```\n\n**Key findings:**\n\n1. **No authentication enforcement**: The `authorization` header is accepted as an optional parameter but **never validated or used** in the endpoint logic.\n\n2. **The endpoint is effectively open**: Anyone can call this endpoint without providing any credentials since:\n   - The `authorization` parameter has a default value of `None`\n   - There's no code that checks or validates this header\n   - The handler proceeds with processing regardless of authentication\n\n3. **Security implications**: This means the API is currently completely open and doesn't require any API key or authentication token to access the `/v1/chat/completions` endpoint.\n\nThis is a significant security concern if the service is meant to be restricted. The authorization header is captured but ignored, making the endpoint publicly accessible to anyone who can reach the server."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":6120,"cache_read_input_tokens":15924,"output_tokens":282,"service_tier":"standard"}},"costUSD":0.159891,"durationMs":13472,"requestId":"req_011CPdJcf9NfmtQLJJ1mCa5Z","type":"assistant","uuid":"822a984e-e3ad-4023-b1e5-17fcd068d82f","timestamp":"2025-05-30T07:16:39.181Z"}
